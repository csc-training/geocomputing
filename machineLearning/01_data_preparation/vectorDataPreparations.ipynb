{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vector data preparations\n",
    "\n",
    "This script prepares the **Paavo zip code dataset** from the Statistics of Finland\n",
    "for machine learning purposes. \n",
    "\n",
    "It reads the original shapefile, scales all the numerical values, joins some auxiliary\n",
    "data and encodes one text field for machine learning purposes. The result is saved as geopackage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from shapely.geometry import Point, MultiPolygon, Polygon\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump, load\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create directories if they do not already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['../data']\n",
    "\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download the Paavo data from Allas with urllib and unzip it to the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlretrieve('https://a3s.fi/gis-courses/gis_ml/paavo.zip', '../data/paavo.zip')\n",
    "\n",
    "with zipfile.ZipFile('../data/paavo.zip', 'r') as zip_file:\n",
    "    zip_file.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_code_shapefile = '../data/paavo/pno_tilasto_2020.shp'\n",
    "finnish_regions_shapefile = '../data/paavo/SuomenMaakuntajako_2020_10k.shp'\n",
    "output_file_path = '../data/paavo/zip_code_data_after_preparation.gpkg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading and cleaning the data\n",
    "\n",
    "Read the zip code dataset into a geopandas dataframe **original_gdf** and drop unnecessary rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the data from a shapefile to a geopandas dataframe\n",
    "original_gdf = gpd.read_file(zip_code_shapefile, encoding='utf-8')\n",
    "print(f\"Original dataframe size: {len(original_gdf.index)} zip codes with {len(original_gdf.columns)} columns\")\n",
    "\n",
    "### Drop all rows that have missing values or where average income is -1 (=not known) or 0\n",
    "original_gdf = original_gdf.dropna()    \n",
    "original_gdf = original_gdf[original_gdf[\"hr_mtu\"]>0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataframe size after dropping some rows: {len(original_gdf.index)} zip codes with {len(original_gdf.columns)} columns\")\n",
    "\n",
    "### Remove some columns that are strings (nanm, kunta = name of the municipality in Finnish and Swedish.\n",
    "### or which might make the modeling too easy ('hr_mtu','hr_tuy','hr_pi_tul','hr_ke_tul','hr_hy_tul','hr_ovy')\n",
    "columns_to_be_removed_completely = ['namn','kunta','hr_ktu','hr_tuy','hr_pi_tul','hr_ke_tul','hr_hy_tul','hr_ovy']\n",
    "original_gdf = original_gdf.drop(columns_to_be_removed_completely,axis=1)\n",
    "\n",
    "print(f\"Dataframe size after dropping some columns: {len(original_gdf.index)} zip codes with {len(original_gdf.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Plot the geodataframe\n",
    "If plotting maps with matplotlib is not familiar. Here are some things you can play with\n",
    "* figsize - different heigh, width\n",
    "* column - try other zip code values\n",
    "* cmap - this is the color map, here are the possibile options https://matplotlib.org/3.3.1/tutorials/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.set_title(\"Average income by zip code\", fontsize=25)\n",
    "ax.set_axis_off()\n",
    "original_gdf.plot(column='hr_mtu', ax=ax, legend=True, cmap=\"magma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scale the numerical columns\n",
    "Most machine learning algorithms benefit from feature scaling which means normalizing the dataset's variablity to values between e.g. 0-1\n",
    "\n",
    "We do this for all numerical columns. Text (string) types of columns need different kind of treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get list of all column headings\n",
    "all_columns = list(original_gdf.columns)\n",
    "\n",
    "### List the column names that we don't want to be scaled\n",
    "col_names_no_scaling = ['postinumer','nimi','hr_mtu','geometry']\n",
    "\n",
    "### List of column names we want to scale. (all columns minus those we don't want)\n",
    "col_names_to_scaling = [column for column in all_columns if column not in col_names_no_scaling]\n",
    "\n",
    "### Subset the data for only those to-be scaled\n",
    "gdf = original_gdf[col_names_to_scaling]\n",
    "\n",
    "### Apply a Scikit StandardScaler for all the columns left in gdf\n",
    "scaler = StandardScaler()\n",
    "scaled_values_array = scaler.fit_transform(gdf)\n",
    "\n",
    "### You could save the scaler for later use with this\n",
    "dump(scaler, 'zip_code_scaler.bin', compress=True)\n",
    "\n",
    "### The scaled columns come back as a numpy ndarray, switch back to a geopandas dataframe again\n",
    "gdf = pd.DataFrame(scaled_values_array)\n",
    "gdf.columns = col_names_to_scaling\n",
    "\n",
    "### Join the non-scaled columns back with the the scaled columns by index\n",
    "scaled_gdf = original_gdf[col_names_no_scaling].join(gdf)\n",
    "scaled_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Encode categorical (text) columns \n",
    "\n",
    "For text and categorical data we need different kind of pre-processing. In this excercise we use the most popular method of one-hot encoding (also called dummy variables) for categorical data. \n",
    "\n",
    "More information on one-hot encoding\n",
    "https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding\n",
    "\n",
    "It might not always be the best option. See other options \n",
    "https://towardsdatascience.com/stop-one-hot-encoding-your-categorical-variables-bbb0fba89809\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Spatially join the region information to the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the regions shapefile and choose only the name of the region and its geometry\n",
    "finnish_regions_gdf = gpd.read_file(finnish_regions_shapefile)\n",
    "finnish_regions_gdf = finnish_regions_gdf[['NAMEFIN','geometry']]\n",
    "\n",
    "### A function we use to return centroid point geometry from a zip code polygon\n",
    "def returnPointGeometryFromXY(polygon_geometry):\n",
    "    ## Calculate x and y of the centroid\n",
    "    centroid_x,centroid_y = polygon_geometry.centroid.x,polygon_geometry.centroid.y\n",
    "    ## Create a shapely Point geometry of the x and y coords\n",
    "    point_geometry = Point(centroid_x,centroid_y)\n",
    "    return point_geometry\n",
    "\n",
    "### Stash the polygon geometry to another column as we are going to overwrite the 'geometry' with centroid geometry\n",
    "scaled_gdf['polygon_geometry'] = scaled_gdf['geometry']\n",
    "\n",
    "### We will be joining the region name to zip codes according to the zip code centroid. \n",
    "### This calls the function above and returns centroid to every row\n",
    "scaled_gdf[\"geometry\"] = scaled_gdf['geometry'].apply(returnPointGeometryFromXY)\n",
    "\n",
    "### Spatially join the region name to the zip codes using the centroid of zip codes and region polygons\n",
    "scaled_gdf = gpd.sjoin(scaled_gdf,finnish_regions_gdf,how='inner',op='intersects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 One-hot encode the region name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Switch the polygon geometry back to the 'geometry' field and drop uselesss columns\n",
    "scaled_gdf['geometry'] = scaled_gdf['polygon_geometry']\n",
    "scaled_gdf.drop(['index_right','polygon_geometry'],axis=1, inplace=True)\n",
    "\n",
    "### Encode the region name with the One-hot encoding (= in pandas, dummy encoding)\n",
    "encoded_gdf = pd.get_dummies(scaled_gdf['NAMEFIN'])\n",
    "\n",
    "### Join scaled gdf and encoded gdf together\n",
    "scaled_and_encoded_gdf = scaled_gdf.join(encoded_gdf).drop('NAMEFIN',axis=1)\n",
    "\n",
    "### The resulting dataframe has Polygon and Multipolygon geometries. \n",
    "### This upcasts the polygons to multipolygon format so all of them have the same format\n",
    "scaled_and_encoded_gdf[\"geometry\"] = [MultiPolygon([feature]) if type(feature) == Polygon else feature for feature in scaled_and_encoded_gdf[\"geometry\"]]\n",
    "print(\"Dataframe size after adding region name: \" + str(len(scaled_and_encoded_gdf.index))+ \" zip codes with \" + str(len(scaled_and_encoded_gdf.columns)) + \" columns\")\n",
    "\n",
    "### Print the tail of the dataframe\n",
    "scaled_and_encoded_gdf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Write the pre-processed zip code data to file as a Geopackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write the prepared zipcode dataset to a geopackage\n",
    "scaled_and_encoded_gdf.to_file(output_file_path, driver=\"GPKG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
